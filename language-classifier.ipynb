{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Build a Machine Learning App from Scratch\n",
    "This an *interactive notebook version* of an article originally published by my friend [Cameron Smith](https://camtsmith.com/about). The original tutorial can be found [here](https://camtsmith.com/articles/2017-10/naive-bayes-text-classification).\n",
    "\n",
    "#### Introduction\n",
    "This project has three steps: \n",
    "1. The first is constructing a corpus of language data.\n",
    "2. The second is training and testing a language classifier model to predict categories. \n",
    "3. The third step is deploying the application to the web along with an API (not covered in this notebook).\n",
    "\n",
    "You can find the complete source code [here](https://github.com/camoverride/language-classifier) and moreover if you’d like a \"sneak peek at what the application looks like in the wild\", click [here](https://language-classifier-app.herokuapp.com/).\n",
    "\n",
    "#### Building a Corpus\n",
    "\n",
    "In order to perform language classification, a data source is needed. A good source will have a large amount of text and accurate category labels. Wikipedia seems like a great place to start. Not only do they have a well-documented API, but it allows for language-specific querying.\n",
    "\n",
    "We'll start off by creating a list of languages to be used (prefixes were taken from [here](https://en.wikipedia.org/wiki/List_of_Wikipedias))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "LANGUAGES = ['en', 'sv', 'de', 'fr', 'nl', 'ru', 'it', 'es', 'pl', 'vi', 'pt', 'uk', 'fa', 'sco']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're using languages with large numbers of articles (but feel free to play with it). Additionally, we are selecting [Scots](https://en.wikipedia.org/wiki/Scots_language), because it’s quite similar to English and will provide an interesting challenge for our algorithm. \n",
    "\n",
    "Let's not forget to import these libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and create two main objects: \n",
    "1. **GetArticles** which can be used to grab data from Wikipedia. It also performs basic document sanitizing, such as removing punctuation, HTML tags, and citation brackets. \n",
    "2. **Database** which creates a SQLite database with two tables but not covered in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetArticles(object):\n",
    "    \"\"\"\n",
    "    This is an object with the public method write_articles(language_id, number_of_articles,\n",
    "    db_location). This writes sanitized articles to text files in a specified location.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "    def _get_random_article_ids(self, language_id, number_of_articles):\n",
    "        \"\"\"\n",
    "        Makes a request for random article ids. \"rnnamespace=0\" means that only articles are chosen,\n",
    "        as opposed to user-talk pages or category pages. These ids are used by the _get_article_text\n",
    "        function to request articles.\n",
    "        \"\"\"\n",
    "        query = \\\n",
    "                        'https://' + language_id \\\n",
    "                        + '.wikipedia.org/w/api.php?format=json&action=query&list=random&rnlimit=' \\\n",
    "                        + str(number_of_articles) + '&rnnamespace=0'\n",
    "\n",
    "        # reads the response into a json object that can be iterated over\n",
    "        data = json.loads(requests.get(query).text)\n",
    "\n",
    "        # collects the ids from the json\n",
    "        ids = []\n",
    "        for article in data['query']['random']:\n",
    "            ids.append(article['id'])\n",
    "\n",
    "        return ids\n",
    "\n",
    "\n",
    "    def _get_article_text(self, language_id, article_id_list):\n",
    "        \"\"\"\n",
    "        This function takes a list of articles and yields a tuple (article_title, article_text).\n",
    "        \"\"\"\n",
    "        for idx in article_id_list:\n",
    "            idx = str(idx)\n",
    "            query = \\\n",
    "                            'https://' + language_id \\\n",
    "                            + '.wikipedia.org/w/api.php?format=json&action=query&prop=extracts&pageids=' \\\n",
    "                            + idx + '&redirects=true'\n",
    "\n",
    "            data = json.loads(requests.get(query).text)\n",
    "\n",
    "            try:\n",
    "                title = data['query']['pages'][idx]['title']\n",
    "                text_body = data['query']['pages'][idx]['extract']\n",
    "            except KeyError as error:\n",
    "                # if nothing is returned for the request, skip to the next item\n",
    "                # if it is important to download a precise number of files\n",
    "                # then this can be repeated for every error to get a new file\n",
    "                # but getting a precise number of files shouldn't matter\n",
    "                print(error)\n",
    "                continue\n",
    "\n",
    "            def clean(text):\n",
    "                \"\"\"\n",
    "                Sanitizes the document by removing HTML tags, citations, and punctuation. This\n",
    "                function can also be expanded to remove headers, footers, side-bar elements, etc.\n",
    "                \"\"\"\n",
    "                match_tag = re.compile(r'(<[^>]+>|\\[\\d+\\]|[,.\\'\\\"()])')\n",
    "                return match_tag.sub('', text)\n",
    "\n",
    "            yield title, clean(text_body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test it with N random articles (recommend to scrap less than 100, but feel free to break things as part of your learning). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palazzo del Magnifico\n",
      "Thomas Batts\n",
      "Hayden Pass\n",
      "McKague\n",
      "Umhlobo Wenene FM\n",
      "Lea Salonga (album)\n",
      "RAF Dishforth\n",
      "Proposed National Unification Promotion Law\n",
      "Yasmine Larsson\n",
      "Plant Information Management System\n"
     ]
    }
   ],
   "source": [
    "# initializing object\n",
    "getdata = GetArticles()\n",
    "# getting ids of N random articles\n",
    "articles = getdata._get_random_article_ids('en', 10)\n",
    "# fetching text\n",
    "text_list = getdata._get_article_text('en', articles)\n",
    "# printing titles\n",
    "for title, text in text_list:\n",
    "    print(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be continued.."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
