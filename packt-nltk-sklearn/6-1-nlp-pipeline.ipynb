{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Pipeline\n",
    "1. sentence tokenize\n",
    "1. word tokenize\n",
    "1. pos tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This strategy has several advantages. it is very low memory scalable to large datasets as there is no need to store a vocabulary dictionary in memory it is fast to pickle and un-pickle as it holds no state besides the constructor parameters it can be used in a streaming (partial fit) or parallel pipeline as there is no state computed during fit.'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "corpus = [\n",
    "    \"\"\"\n",
    "    This strategy has several advantages.\n",
    "    it is very low memory scalable to large datasets as there is no need to store a vocabulary dictionary in memory\n",
    "    it is fast to pickle and un-pickle as it holds no state besides the constructor parameters\n",
    "    it can be used in a streaming (partial fit) or parallel pipeline as there is no state computed during fit.\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    It turns a collection of text documents into a scipy.sparse matrix holding token occurrence counts (or binary occurrence information), \n",
    "    possibly normalized as token frequencies if norm=’l1’ or projected on the euclidean unit sphere if norm=’l2’.\n",
    "    \"\"\"]\n",
    "example = re.sub(r'\\n *', ' ', corpus[0]).strip()\n",
    "example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old school approach\n",
    "Needs to store partial result in memory, not suitable for large datasets or streaming pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('This', 'DT'),\n",
       "  ('strategy', 'NN'),\n",
       "  ('has', 'VBZ'),\n",
       "  ('several', 'JJ'),\n",
       "  ('advantages', 'NNS'),\n",
       "  ('.', '.')],\n",
       " [('it', 'PRP'),\n",
       "  ('is', 'VBZ'),\n",
       "  ('very', 'RB'),\n",
       "  ('low', 'JJ'),\n",
       "  ('memory', 'NN'),\n",
       "  ('scalable', 'NN'),\n",
       "  ('to', 'TO'),\n",
       "  ('large', 'JJ'),\n",
       "  ('datasets', 'NNS'),\n",
       "  ('as', 'IN'),\n",
       "  ('there', 'EX'),\n",
       "  ('is', 'VBZ'),\n",
       "  ('no', 'DT'),\n",
       "  ('need', 'NN'),\n",
       "  ('to', 'TO'),\n",
       "  ('store', 'VB'),\n",
       "  ('a', 'DT'),\n",
       "  ('vocabulary', 'JJ'),\n",
       "  ('dictionary', 'NN'),\n",
       "  ('in', 'IN'),\n",
       "  ('memory', 'NN'),\n",
       "  ('it', 'PRP'),\n",
       "  ('is', 'VBZ'),\n",
       "  ('fast', 'JJ'),\n",
       "  ('to', 'TO'),\n",
       "  ('pickle', 'VB'),\n",
       "  ('and', 'CC'),\n",
       "  ('un-pickle', 'JJ'),\n",
       "  ('as', 'IN'),\n",
       "  ('it', 'PRP'),\n",
       "  ('holds', 'VBZ'),\n",
       "  ('no', 'DT'),\n",
       "  ('state', 'NN'),\n",
       "  ('besides', 'IN'),\n",
       "  ('the', 'DT'),\n",
       "  ('constructor', 'NN'),\n",
       "  ('parameters', 'NNS'),\n",
       "  ('it', 'PRP'),\n",
       "  ('can', 'MD'),\n",
       "  ('be', 'VB'),\n",
       "  ('used', 'VBN'),\n",
       "  ('in', 'IN'),\n",
       "  ('a', 'DT'),\n",
       "  ('streaming', 'NN'),\n",
       "  ('(', '('),\n",
       "  ('partial', 'JJ'),\n",
       "  ('fit', 'NN'),\n",
       "  (')', ')'),\n",
       "  ('or', 'CC'),\n",
       "  ('parallel', 'JJ'),\n",
       "  ('pipeline', 'NN'),\n",
       "  ('as', 'IN'),\n",
       "  ('there', 'EX'),\n",
       "  ('is', 'VBZ'),\n",
       "  ('no', 'DT'),\n",
       "  ('state', 'NN'),\n",
       "  ('computed', 'VBD'),\n",
       "  ('during', 'IN'),\n",
       "  ('fit', 'NN'),\n",
       "  ('.', '.')]]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = nltk.sent_tokenize(example)\n",
    "tokens = [nltk.word_tokenize(s) for s in sentences]\n",
    "[nltk.tag.pos_tag(t) for t in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decorators approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(func):\n",
    "    '''pipeline decorator that calls function func(), next() and returns'''\n",
    "    def start_pipeline(*args, **kwargs):\n",
    "        iterator = func(*args, **kwargs)\n",
    "        next(iterator)\n",
    "        return iterator\n",
    "    return start_pipeline\n",
    "\n",
    "def ingest(corpus, target):\n",
    "    for text in corpus:\n",
    "        target.send(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining pipeline blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pipeline\n",
    "def tokenize_sentence(targets):\n",
    "    while True:\n",
    "        text = (yield)  # (yield) gets an item from an upstream step\n",
    "        sentences = nltk.sent_tokenize(text)\n",
    "        for sentence in sentences:\n",
    "            for target in targets:\n",
    "                target.send(sentence)  # send() sends data downstream\n",
    "\n",
    "@pipeline\n",
    "def tokenize_words(targets):\n",
    "    while True:\n",
    "        sentence = (yield)\n",
    "        words = nltk.word_tokenize(sentence)\n",
    "        for target in targets:\n",
    "            target.send(words)\n",
    "\n",
    "@pipeline\n",
    "def pos_tagging(targets):\n",
    "    while True:\n",
    "        words = (yield)\n",
    "        tagged_words = nltk.pos_tag(words)\n",
    "        for target in targets:\n",
    "            target.send(tagged_words)\n",
    "\n",
    "@pipeline\n",
    "def printline(line):\n",
    "    while True:\n",
    "        input = (yield)\n",
    "        print('\\n' + line)\n",
    "        print(input)\n",
    "\n",
    "@pipeline\n",
    "def collect():\n",
    "    while True:\n",
    "        input = (yield)\n",
    "        #myList += input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentence:\n",
      "\n",
      "    This strategy has several advantages.\n",
      "\n",
      "Word tokens:\n",
      "['This', 'strategy', 'has', 'several', 'advantages', '.']\n",
      "\n",
      "Tokens-Tags:\n",
      "[('This', 'DT'), ('strategy', 'NN'), ('has', 'VBZ'), ('several', 'JJ'), ('advantages', 'NNS'), ('.', '.')]\n",
      "\n",
      "Sentence:\n",
      "it is very low memory scalable to large datasets as there is no need to store a vocabulary dictionary in memory\n",
      "    it is fast to pickle and un-pickle as it holds no state besides the constructor parameters\n",
      "    it can be used in a streaming (partial fit) or parallel pipeline as there is no state computed during fit.\n",
      "\n",
      "Word tokens:\n",
      "['it', 'is', 'very', 'low', 'memory', 'scalable', 'to', 'large', 'datasets', 'as', 'there', 'is', 'no', 'need', 'to', 'store', 'a', 'vocabulary', 'dictionary', 'in', 'memory', 'it', 'is', 'fast', 'to', 'pickle', 'and', 'un-pickle', 'as', 'it', 'holds', 'no', 'state', 'besides', 'the', 'constructor', 'parameters', 'it', 'can', 'be', 'used', 'in', 'a', 'streaming', '(', 'partial', 'fit', ')', 'or', 'parallel', 'pipeline', 'as', 'there', 'is', 'no', 'state', 'computed', 'during', 'fit', '.']\n",
      "\n",
      "Tokens-Tags:\n",
      "[('it', 'PRP'), ('is', 'VBZ'), ('very', 'RB'), ('low', 'JJ'), ('memory', 'NN'), ('scalable', 'NN'), ('to', 'TO'), ('large', 'JJ'), ('datasets', 'NNS'), ('as', 'IN'), ('there', 'EX'), ('is', 'VBZ'), ('no', 'DT'), ('need', 'NN'), ('to', 'TO'), ('store', 'VB'), ('a', 'DT'), ('vocabulary', 'JJ'), ('dictionary', 'NN'), ('in', 'IN'), ('memory', 'NN'), ('it', 'PRP'), ('is', 'VBZ'), ('fast', 'JJ'), ('to', 'TO'), ('pickle', 'VB'), ('and', 'CC'), ('un-pickle', 'JJ'), ('as', 'IN'), ('it', 'PRP'), ('holds', 'VBZ'), ('no', 'DT'), ('state', 'NN'), ('besides', 'IN'), ('the', 'DT'), ('constructor', 'NN'), ('parameters', 'NNS'), ('it', 'PRP'), ('can', 'MD'), ('be', 'VB'), ('used', 'VBN'), ('in', 'IN'), ('a', 'DT'), ('streaming', 'NN'), ('(', '('), ('partial', 'JJ'), ('fit', 'NN'), (')', ')'), ('or', 'CC'), ('parallel', 'JJ'), ('pipeline', 'NN'), ('as', 'IN'), ('there', 'EX'), ('is', 'VBZ'), ('no', 'DT'), ('state', 'NN'), ('computed', 'VBD'), ('during', 'IN'), ('fit', 'NN'), ('.', '.')]\n",
      "\n",
      "Sentence:\n",
      "\n",
      "    It turns a collection of text documents into a scipy.sparse matrix holding token occurrence counts (or binary occurrence information), \n",
      "    possibly normalized as token frequencies if norm=’l1’ or projected on the euclidean unit sphere if norm=’l2’.\n",
      "\n",
      "Word tokens:\n",
      "['It', 'turns', 'a', 'collection', 'of', 'text', 'documents', 'into', 'a', 'scipy.sparse', 'matrix', 'holding', 'token', 'occurrence', 'counts', '(', 'or', 'binary', 'occurrence', 'information', ')', ',', 'possibly', 'normalized', 'as', 'token', 'frequencies', 'if', 'norm=', '’', 'l1', '’', 'or', 'projected', 'on', 'the', 'euclidean', 'unit', 'sphere', 'if', 'norm=', '’', 'l2', '’', '.']\n",
      "\n",
      "Tokens-Tags:\n",
      "[('It', 'PRP'), ('turns', 'VBZ'), ('a', 'DT'), ('collection', 'NN'), ('of', 'IN'), ('text', 'JJ'), ('documents', 'NNS'), ('into', 'IN'), ('a', 'DT'), ('scipy.sparse', 'JJ'), ('matrix', 'NN'), ('holding', 'VBG'), ('token', 'JJ'), ('occurrence', 'NN'), ('counts', 'NNS'), ('(', '('), ('or', 'CC'), ('binary', 'JJ'), ('occurrence', 'NN'), ('information', 'NN'), (')', ')'), (',', ','), ('possibly', 'RB'), ('normalized', 'VBN'), ('as', 'IN'), ('token', 'JJ'), ('frequencies', 'NNS'), ('if', 'IN'), ('norm=', 'JJ'), ('’', 'NNP'), ('l1', 'NN'), ('’', 'NN'), ('or', 'CC'), ('projected', 'VBN'), ('on', 'IN'), ('the', 'DT'), ('euclidean', 'JJ'), ('unit', 'NN'), ('sphere', 'RB'), ('if', 'IN'), ('norm=', 'JJ'), ('’', 'NNP'), ('l2', 'NN'), ('’', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "ingest(corpus, \n",
    "    tokenize_sentence([\n",
    "        printline('Sentence:'),\n",
    "        tokenize_words([\n",
    "            printline('Word tokens:'),\n",
    "            pos_tagging([printline('Tokens-Tags:'), collect()])\n",
    "        ])\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
